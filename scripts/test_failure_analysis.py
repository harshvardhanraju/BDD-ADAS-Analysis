#!/usr/bin/env python3
"""
Test Failure Analysis System (Phase 3)

This script tests the comprehensive failure analysis and pattern detection system.
"""

import sys
import json
import numpy as np
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.evaluation.analysis import FailureAnalyzer, PerformancePatternDetector
from src.evaluation.visualization import DetectionVisualizer


def create_sample_evaluation_data():
    """Create comprehensive sample evaluation data for testing."""
    
    # Sample COCO metrics
    coco_metrics = {
        'mAP': 0.342,
        'mAP@0.5': 0.589,
        'mAP@0.75': 0.371,
        'mAP_small': 0.125,
        'mAP_medium': 0.398,
        'mAP_large': 0.524,
        'safety_critical_mAP': 0.289,
        'per_class_AP': {
            'pedestrian': 0.456,
            'rider': 0.234,
            'car': 0.678,
            'truck': 0.567,
            'bus': 0.489,
            'train': 0.123,
            'motorcycle': 0.345,
            'bicycle': 0.198,
            'traffic_light': 0.567,
            'traffic_sign': 0.678
        }
    }
    
    # Sample safety metrics
    safety_metrics = {
        'overall_safety_score': {
            'overall_safety_score': 0.678,
            'weighted_recall': 0.723,
            'safety_compliance': False
        },
        'per_class_safety': {
            'pedestrian': {
                'precision': 0.789,
                'recall': 0.654,
                'f1_score': 0.715,
                'false_negative_rate': 0.346,
                'safety_risk_level': 'MEDIUM'
            },
            'rider': {
                'precision': 0.456,
                'recall': 0.423,
                'f1_score': 0.439,
                'false_negative_rate': 0.577,
                'safety_risk_level': 'HIGH'
            },
            'bicycle': {
                'precision': 0.345,
                'recall': 0.289,
                'f1_score': 0.315,
                'false_negative_rate': 0.711,
                'safety_risk_level': 'HIGH'
            },
            'motorcycle': {
                'precision': 0.567,
                'recall': 0.512,
                'f1_score': 0.538,
                'false_negative_rate': 0.488,
                'safety_risk_level': 'MEDIUM'
            }
        }
    }
    
    # Sample contextual metrics
    contextual_metrics = {
        'weather_performance': {
            'clear': {'mean_ap': 0.456, 'count': 150},
            'overcast': {'mean_ap': 0.398, 'count': 89},
            'rainy': {'mean_ap': 0.287, 'count': 45},
            'snowy': {'mean_ap': 0.198, 'count': 23}
        },
        'lighting_performance': {
            'daytime': {'mean_ap': 0.423, 'count': 200},
            'dawn/dusk': {'mean_ap': 0.345, 'count': 67},
            'night': {'mean_ap': 0.234, 'count': 78}
        },
        'scene_performance': {
            'highway': {'mean_ap': 0.489, 'count': 120},
            'city_street': {'mean_ap': 0.367, 'count': 156},
            'residential': {'mean_ap': 0.401, 'count': 89},
            'parking_lot': {'mean_ap': 0.345, 'count': 45}
        },
        'position_analysis': {
            'center': {'mean_ap': 0.445, 'count': 180},
            'edge': {'mean_ap': 0.323, 'count': 125},
            'corner': {'mean_ap': 0.289, 'count': 67}
        }
    }
    
    # Sample failure analysis data (would be generated by FailureAnalyzer)
    sample_failures = []
    
    # Create sample ground truth and predictions for failure analysis
    for i in range(20):
        # Sample ground truth
        gt = []
        for j in range(np.random.randint(1, 5)):  # 1-4 objects per image
            gt.append({
                'category_id': np.random.randint(0, 10),
                'bbox': [
                    np.random.randint(0, 800),  # x
                    np.random.randint(0, 600),  # y
                    np.random.randint(50, 200), # width
                    np.random.randint(50, 200)  # height
                ],
                'area': np.random.randint(2500, 40000),
                'image_id': f'test_img_{i:03d}',
                'weather': np.random.choice(['clear', 'overcast', 'rainy', 'snowy']),
                'lighting': np.random.choice(['daytime', 'dawn/dusk', 'night']),
                'scene_type': np.random.choice(['highway', 'city_street', 'residential'])
            })
        
        # Sample predictions (with some matching, some not)
        preds = []
        for gt_obj in gt:
            # Create matching prediction with some noise
            if np.random.random() > 0.3:  # 70% recall
                pred_bbox = gt_obj['bbox'].copy()
                # Add noise to bbox
                pred_bbox[0] += np.random.randint(-20, 20)
                pred_bbox[1] += np.random.randint(-20, 20)
                pred_bbox[2] += np.random.randint(-10, 10)
                pred_bbox[3] += np.random.randint(-10, 10)
                
                # Sometimes wrong class
                pred_class = gt_obj['category_id']
                if np.random.random() < 0.1:  # 10% classification error
                    pred_class = np.random.randint(0, 10)
                
                preds.append({
                    'category_id': pred_class,
                    'bbox': pred_bbox,
                    'score': np.random.beta(8, 2),  # Higher confidence scores
                    'image_id': gt_obj['image_id']
                })
        
        # Add some false positives
        for _ in range(np.random.randint(0, 3)):
            preds.append({
                'category_id': np.random.randint(0, 10),
                'bbox': [
                    np.random.randint(0, 800),
                    np.random.randint(0, 600),
                    np.random.randint(50, 200),
                    np.random.randint(50, 200)
                ],
                'score': np.random.beta(3, 5),  # Lower confidence for FPs
                'image_id': f'test_img_{i:03d}'
            })
        
        sample_failures.append({
            'image_id': f'test_img_{i:03d}',
            'ground_truth': gt,
            'predictions': preds,
            'metadata': {
                'weather': gt[0]['weather'] if gt else 'clear',
                'lighting': gt[0]['lighting'] if gt else 'daytime',
                'scene_type': gt[0]['scene_type'] if gt else 'highway'
            }
        })
    
    return {
        'coco_metrics': coco_metrics,
        'safety_metrics': safety_metrics,
        'contextual_metrics': contextual_metrics,
        'sample_predictions_data': sample_failures
    }


def test_failure_analyzer():
    """Test the FailureAnalyzer component."""
    print("Testing Failure Analysis System...")
    
    # Create failure analyzer
    analyzer = FailureAnalyzer()
    
    # Create sample data
    eval_data = create_sample_evaluation_data()
    
    # Flatten the nested structure to match expected interface
    all_ground_truths = []
    all_predictions = []
    image_metadata = []
    
    for sample in eval_data['sample_predictions_data']:
        # Add ground truth annotations
        for gt in sample['ground_truth']:
            all_ground_truths.append(gt)
        
        # Add predictions
        for pred in sample['predictions']:
            all_predictions.append(pred)
        
        # Add metadata
        image_metadata.append({
            'image_id': sample['image_id'],
            **sample['metadata']
        })
    
    # Run failure analysis
    print("  Running failure analysis...")
    failure_results = analyzer.analyze_failures(
        all_predictions,
        all_ground_truths, 
        image_metadata
    )
    
    # Display results
    print("  âœ… Failure Analysis Results:")
    print(f"    - Total failures analyzed: {failure_results['summary']['total_failures']}")
    print(f"    - False negatives: {failure_results['summary']['false_negatives']}")
    print(f"    - False positives: {failure_results['summary']['false_positives']}")
    print(f"    - Classification errors: {failure_results['summary']['classification_errors']}")
    print(f"    - Localization errors: {failure_results['summary']['localization_errors']}")
    
    # Test pattern identification
    if 'failure_patterns' in failure_results:
        patterns = failure_results['failure_patterns']
        print("  âœ… Failure Patterns Identified:")
        for pattern_type, pattern_data in patterns.items():
            if isinstance(pattern_data, dict) and pattern_data:
                print(f"    - {pattern_type}: {len(pattern_data)} patterns found")
    
    return failure_results


def test_pattern_detector():
    """Test the PerformancePatternDetector component."""
    print("\\nTesting Performance Pattern Detection...")
    
    # Create pattern detector
    detector = PerformancePatternDetector()
    
    # Create comprehensive evaluation data
    eval_data = create_sample_evaluation_data()
    
    # Run pattern detection
    print("  Detecting performance patterns...")
    patterns = detector.detect_performance_patterns(eval_data)
    
    # Display results
    print("  âœ… Pattern Detection Results:")
    
    # Class clustering results
    if 'class_performance_clusters' in patterns and 'clusters' in patterns['class_performance_clusters']:
        clusters = patterns['class_performance_clusters']['clusters']
        print(f"    - Class clusters identified: {len(clusters)}")
        for cluster_id, cluster_data in clusters.items():
            print(f"      * {cluster_id}: {len(cluster_data['classes'])} classes, "
                  f"tier: {cluster_data['performance_tier']}")
    
    # Environmental patterns
    if 'environmental_impact' in patterns:
        env_patterns = patterns['environmental_impact']
        print(f"    - Environmental patterns: {len(env_patterns)} condition types analyzed")
        for condition_type, data in env_patterns.items():
            if 'stability_assessment' in data:
                print(f"      * {condition_type}: {data['stability_assessment']} performance")
    
    # Safety patterns
    if 'safety_risk_patterns' in patterns and 'high_risk_classes' in patterns['safety_risk_patterns']:
        high_risk = patterns['safety_risk_patterns']['high_risk_classes']
        compliance = patterns['safety_risk_patterns']['safety_compliance']
        print(f"    - Safety analysis: {'Compliant' if compliance else 'Non-compliant'}")
        if high_risk:
            print(f"      * High-risk classes: {', '.join(high_risk)}")
    
    # Actionable insights
    if 'actionable_insights' in patterns:
        insights = patterns['actionable_insights']
        print(f"    - Actionable insights generated: {len(insights)}")
        
        # Show high priority insights
        high_priority = [i for i in insights if i['priority'] in ['critical', 'high']]
        if high_priority:
            print("      * High Priority Insights:")
            for insight in high_priority[:3]:  # Show top 3
                print(f"        - {insight['insight']}")
    
    return patterns


def test_visualization_generation():
    """Test pattern visualization generation."""
    print("\\nTesting Pattern Visualizations...")
    
    # Create sample data and detect patterns
    eval_data = create_sample_evaluation_data()
    detector = PerformancePatternDetector()
    patterns = detector.detect_performance_patterns(eval_data)
    
    # Generate visualizations
    output_dir = Path("evaluation_results/failure_analysis_tests")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("  Generating pattern visualizations...")
    generated_files = detector.generate_pattern_visualizations(patterns, output_dir)
    
    print("  âœ… Visualization Generation Results:")
    if generated_files:
        for file_path in generated_files:
            print(f"    - Generated: {Path(file_path).name}")
    else:
        print("    - No visualizations generated (may be due to missing data)")
    
    return generated_files


def test_integration():
    """Test integration of failure analyzer and pattern detector."""
    print("\\nTesting Integrated Analysis...")
    
    # Create comprehensive test data
    eval_data = create_sample_evaluation_data()
    
    # Flatten the nested structure to match expected interface
    all_ground_truths = []
    all_predictions = []
    image_metadata = []
    
    for sample in eval_data['sample_predictions_data']:
        # Add ground truth annotations
        for gt in sample['ground_truth']:
            all_ground_truths.append(gt)
        
        # Add predictions
        for pred in sample['predictions']:
            all_predictions.append(pred)
        
        # Add metadata
        image_metadata.append({
            'image_id': sample['image_id'],
            **sample['metadata']
        })
    
    # Run failure analysis
    analyzer = FailureAnalyzer()
    failure_results = analyzer.analyze_failures(
        all_predictions,
        all_ground_truths,
        image_metadata
    )
    
    # Add failure results to evaluation data
    eval_data['failure_analysis'] = failure_results
    
    # Run pattern detection on comprehensive data
    detector = PerformancePatternDetector()
    patterns = detector.detect_performance_patterns(eval_data)
    
    # Generate comprehensive report
    output_dir = Path("evaluation_results/failure_analysis_tests")
    
    # Save comprehensive results
    comprehensive_results = {
        'evaluation_metrics': {
            'coco_metrics': eval_data['coco_metrics'],
            'safety_metrics': eval_data['safety_metrics'],
            'contextual_metrics': eval_data['contextual_metrics']
        },
        'failure_analysis': failure_results,
        'performance_patterns': patterns,
        'analysis_metadata': {
            'test_dataset_size': len(eval_data['sample_predictions_data']),
            'analysis_components': ['failure_analyzer', 'pattern_detector'],
            'test_timestamp': '2024-01-01T00:00:00Z'
        }
    }
    
    # Save results to JSON
    results_file = output_dir / "comprehensive_failure_analysis_results.json"
    with open(results_file, 'w') as f:
        json.dump(comprehensive_results, f, indent=2)
    
    print("  âœ… Integration Test Results:")
    print(f"    - Comprehensive analysis completed")
    print(f"    - Results saved to: {results_file}")
    print(f"    - Total insights generated: {len(patterns.get('actionable_insights', []))}")
    
    # Show critical insights
    critical_insights = [i for i in patterns.get('actionable_insights', []) 
                        if i['priority'] == 'critical']
    if critical_insights:
        print("    - Critical Insights:")
        for insight in critical_insights:
            print(f"      * {insight['insight']}")
    
    return comprehensive_results


def main():
    """Run all failure analysis tests."""
    print("ðŸ” Testing BDD100K Failure Analysis System (Phase 3)")
    print("=" * 65)
    
    try:
        # Test individual components
        failure_results = test_failure_analyzer()
        
        pattern_results = test_pattern_detector()
        
        visualization_files = test_visualization_generation()
        
        # Test integration
        integrated_results = test_integration()
        
        print("\\n" + "=" * 65)
        print("âœ… All Phase 3 failure analysis tests completed successfully!")
        print("\\nðŸ“ Generated outputs:")
        print("  - evaluation_results/failure_analysis_tests/")
        print("    * comprehensive_failure_analysis_results.json")
        print("    * Pattern visualization charts (if generated)")
        
        print("\\nðŸŽ¯ Phase 3 Status: COMPLETED")
        print("Ready to proceed to Phase 4: Performance clustering and pattern detection")
        
        return True
        
    except Exception as e:
        print(f"\\nâŒ Phase 3 test failed: {e}")
        raise


if __name__ == "__main__":
    main()